# 第四章：微调技术

[← 上一章：模型训练基础](../ch03-training/README.md) | [返回目录](../../README.md)

---

## 🎯 本章学习目标

- 理解迁移学习和微调的原理
- 掌握全量微调和参数高效微调
- 学习LoRA、Adapter等技术
- 了解Prompt Engineering方法

---

## 🔄 4.1 迁移学习与微调

### 核心思想

```
🎓 迁移学习流程：

预训练阶段：
海量通用数据 → 通用语言模型
(维基百科、书籍、网页等)

        ⬇️ 迁移

微调阶段：
少量特定数据 → 领域专用模型
(医疗、法律、金融等)
```

### 为什么有效？

```
💡 原理：

底层特征（通用）：
- 语法规则
- 词汇语义
- 常识知识
  ⬇️ 已在预训练中学到

高层特征（特定）：
- 领域术语
- 任务模式
- 风格偏好
  ⬇️ 通过微调学习

形象比喻：
预训练 = 打好基础
微调   = 专业培训
```

### 微调的优势

```
✅ vs 从头训练：
- 需要更少的数据
- 训练时间更短
- 效果通常更好
- 计算成本更低

数据量对比：
从头训练：数TB文本
微调：    数GB甚至MB文本
```

---

## 🔧 4.2 全量微调 (Full Fine-tuning)

### 方法

```
🎯 更新所有参数：

预训练模型（冻结）→ 解冻所有层 → 在新数据上训练

θ_finetuned = θ_pretrained + Δθ

其中 Δθ 通过梯度下降得到
```

### 训练设置

```
📋 典型配置：

学习率：    2e-5 到 5e-5  (比预训练小)
Epochs：    3-5           (避免过拟合)
Batch size： 8-32          (根据任务和GPU)
Warmup：    前10%步数

原则：
小学习率 + 少epoch = 保留预训练知识
```

### 适用场景

```
✓ 适合：
- 有足够的标注数据（>10K样本）
- 任务与预训练差异大
- 有充足的计算资源

✗ 不适合：
- 数据很少（容易过拟合）
- 计算资源有限
- 需要训练多个任务
```

---

## 💡 4.3 参数高效微调 (PEFT)

### 动机

```
🤔 全量微调的问题：

1. 内存占用大
   175B模型 → 需要700GB+ GPU内存

2. 训练成本高
   更新所有参数 → 计算量大

3. 存储成本高
   每个任务一个完整模型 → 存储空间大

💡 解决方案：
只训练一小部分参数
```

### PEFT优势

```
✅ 好处：
- 内存占用小（可在消费级GPU训练）
- 训练速度快
- 多任务部署方便
- 避免灾难性遗忘

数字对比：
全量微调：   更新100%参数
PEFT：       更新0.1%-1%参数
效果：       相近或更好！
```

---

## 🎨 4.4 LoRA (Low-Rank Adaptation)

### 核心思想

```
💡 低秩分解：

原始更新：
W' = W + ΔW

LoRA方式：
W' = W + B × A

其中：
W: d × d (原始权重，冻结)
B: d × r
A: r × d
r << d (低秩)

例如：
d = 4096, r = 8
参数量：4096² vs 4096×8×2
减少：99.9% ✓
```

### 图解

```
📊 LoRA结构：

输入 x (d维)
    ⬇️
    ├─→ W·x (冻结路径)
    │
    └─→ B·(A·x) (LoRA路径)
    ⬇️
   相加
    ⬇️
  输出 (d维)

W: 预训练权重（冻结🔒）
A, B: 可训练矩阵（训练🔥）
```

### 实现细节

```
🔧 应用位置：
通常应用于注意力层：
- Query投影矩阵 W_q
- Value投影矩阵 W_v
- (可选) Key投影 W_k
- (可选) 输出投影 W_o

前馈层也可以应用LoRA

💻 伪代码：
class LoRALayer:
    def __init__(self, W, r):
        self.W = W.freeze()  # 冻结原始权重
        self.A = random_init(d, r)
        self.B = zeros(r, d)
        self.alpha = 16  # 缩放因子
    
    def forward(self, x):
        # 原始输出 + LoRA增量
        return self.W @ x + (self.alpha/r) * (self.B @ (self.A @ x))
```

### 超参数选择

```
📋 关键参数：

r (秩)：
- 常用值：4, 8, 16, 32
- 越大：表达能力越强，参数越多
- 越小：效率越高，可能欠拟合

alpha (缩放)：
- 常用值：16, 32
- 控制LoRA贡献的权重
- alpha/r 是实际缩放因子

推荐起点：
r = 8, alpha = 16
```

---

## 🔌 4.5 Adapter

### 结构

```
🏗️ Adapter模块：

        输入 (d维)
          ⬇️
    [LayerNorm] (可选)
          ⬇️
    [Down Project] d → m
          ⬇️
      [非线性激活]
          ⬇️
    [Up Project] m → d
          ⬇️
      [残差连接] ← ─┐
          ⬇️         │
        输出 ─ ─ ─ ─ ┘

瓶颈维度：m << d (如 m = 64)
```

### 插入位置

```
📍 在Transformer层中：

[Multi-Head Attention]
         ⬇️
    [Adapter] ← 插入
         ⬇️
    [LayerNorm]
         ⬇️
[Feed-Forward Network]
         ⬇️
    [Adapter] ← 插入
         ⬇️
    [LayerNorm]
```

### 特点

```
✅ 优势：
- 顺序架构，易于理解
- 参数量小（0.5-5%）
- 不改变预训练权重

⚠️ 注意：
- 增加推理延迟（串行计算）
- 每次前向传播都要经过Adapter

vs LoRA：
Adapter：串行，增加深度
LoRA：   并行，不增加延迟
```

---

## 🎯 4.6 Prefix Tuning

### 核心思想

```
💡 在输入前添加可训练的前缀：

原始输入：
[x₁, x₂, x₃, ...]

添加前缀后：
[P₁, P₂, ..., Pₖ, x₁, x₂, x₃, ...]
 ⬆️  ⬆️       ⬆️
可训练的"虚拟token"

前缀引导模型行为
```

### 实现方式

```
📊 两种实现：

1. Prefix Tuning (原始)：
   - 在每一层添加前缀
   - key和value都添加

2. Prompt Tuning (简化)：
   - 只在输入层添加
   - 更简单，效果相近

结构：
Layer 1: [P₁ᵏ, P₁ᵛ] + [K, V]
Layer 2: [P₂ᵏ, P₂ᵛ] + [K, V]
...
```

### 参数量

```
📈 计算：

前缀长度：k = 20 tokens
层数：    L = 24
隐藏维度：d = 1024

参数量 = k × L × d × 2 (key和value)
       = 20 × 24 × 1024 × 2
       ≈ 1M 参数

vs 模型总参数：175B
比例：0.0006% ✓
```

---

## 💬 4.7 Prompt Engineering

### 什么是Prompt？

```
📝 定义：
给模型的输入指令，用于引导输出

例子：
任务：情感分类

❌ 简单Prompt：
"这部电影很好看"

✅ 好的Prompt：
"请分析以下电影评论的情感倾向（正面/负面/中性）：
评论：这部电影很好看
情感："
```

### Zero-shot vs Few-shot

```
🎯 Zero-shot（零样本）：
不提供示例，直接完成任务

Prompt：
"将下面的英文翻译成中文：
Hello, world!"

─────────────────────

🎯 Few-shot（少样本）：
提供几个示例，再完成任务

Prompt：
"将英文翻译成中文：

英文：Good morning
中文：早上好

英文：Thank you
中文：谢谢

英文：Hello, world!
中文："

Few-shot通常效果更好
```

### Prompt设计技巧

```
✅ 最佳实践：

1. 清晰的指令
   "请用一句话总结以下文章："
   而不是"总结："

2. 提供格式示例
   "输出格式：{标题}-{作者}-{年份}"

3. 设定角色
   "你是一个专业的Python程序员..."

4. 逐步思考 (Chain-of-Thought)
   "让我们一步步分析：
    1. 首先...
    2. 然后...
    3. 最后..."

5. 添加约束
   "用不超过50字回答"
   "使用专业术语"
```

### 思维链 (Chain-of-Thought)

```
💭 让模型展示推理过程：

❌ 直接提问：
"问题：Roger有5个球，他买了2罐球，
每罐3个球。他现在有多少个球？
答案："

✅ CoT Prompt：
"问题：Roger有5个球，他买了2罐球，
每罐3个球。他现在有多少个球？
让我们一步步思考：
1. Roger原本有5个球
2. 他买了2罐，每罐3个球，所以买了 2 × 3 = 6个球
3. 总共：5 + 6 = 11个球
答案：11个球

问题：Jason有20颗糖果...
让我们一步步思考："

效果：
复杂推理任务准确率大幅提升✓
```

---

## 📊 4.8 方法对比

### 参数量与性能

```
┌─────────────┬─────────┬─────────┬─────────┐
│   方法      │ 参数量  │  内存   │  效果   │
├─────────────┼─────────┼─────────┼─────────┤
│ Full FT     │  100%   │  ⭐⭐⭐  │  ⭐⭐⭐⭐ │
│ Adapter     │  0.5-5% │  ⭐⭐⭐⭐ │  ⭐⭐⭐  │
│ LoRA        │  0.1-1% │  ⭐⭐⭐⭐⭐│  ⭐⭐⭐⭐ │
│ Prefix      │  <0.1%  │  ⭐⭐⭐⭐⭐│  ⭐⭐⭐  │
│ Prompt Only │   0%    │  ⭐⭐⭐⭐⭐│  ⭐⭐   │
└─────────────┴─────────┴─────────┴─────────┘
```

### 选择指南

```
🎯 如何选择？

数据充足 (>10K) + 资源充足：
→ Full Fine-tuning

数据中等 + 资源有限：
→ LoRA (推荐)
   - 效果好
   - 效率高
   - 易于部署

数据很少 (<1K)：
→ Prompt Engineering + Few-shot

需要多任务：
→ Adapter 或 LoRA
   - 基础模型共享
   - 每个任务独立模块

极低资源：
→ Prompt Tuning
```

---

## 🛠️ 4.9 实践建议

### 微调流程

```
📋 Step-by-step：

1. 准备数据
   - 清洗和格式化
   - 划分训练/验证/测试集
   - 数据增强（如有必要）

2. 选择基础模型
   - 考虑任务类型
   - 考虑模型大小
   - 考虑语言支持

3. 选择微调方法
   - 根据资源和数据量
   - 参考本章对比

4. 超参数调优
   - 学习率最重要
   - 从小学习率开始
   - 监控验证集

5. 评估和迭代
   - 任务相关指标
   - 人工评估
   - 错误分析
```

### 避免过拟合

```
⚠️ 过拟合信号：
- 训练loss持续下降
- 验证loss上升或不变
- 训练集表现好，测试集差

🛡️ 防止策略：
1. Early Stopping
   - 监控验证集
   - 保存最佳模型

2. 数据增强
   - 回译（翻译后再翻译回来）
   - 同义词替换
   - 随机删除/插入

3. 正则化
   - Dropout (0.1-0.3)
   - Weight decay
   - 减少训练epochs

4. 增加数据
   - 收集更多样本
   - 使用数据增强
```

---

## 💡 关键要点总结

### 核心概念

1. **微调 = 迁移学习**
   ```
   预训练知识 + 任务特定调整
   ```

2. **PEFT = 效率与效果的平衡**
   ```
   少量参数 → 接近全量微调效果
   ```

3. **LoRA = 推荐首选**
   ```
   - 参数少（0.1-1%）
   - 效果好
   - 无推理延迟
   ```

4. **Prompt = 无需训练的适配**
   ```
   通过输入设计引导模型
   ```

### 方法选择树

```
                    开始
                     ↓
               有标注数据？
              ↙        ↘
            是          否
            ↓           ↓
        数据量？    Prompt Engineering
       ↙    ↘
      多     少
      ↓      ↓
   资源充足？ LoRA/Adapter
   ↙    ↘
  是     否
  ↓      ↓
全量微调 LoRA
```

---

## 🎯 思考题

1. **原理理解**
   - LoRA的低秩假设合理吗？为什么？
   - 为什么Adapter会增加延迟但LoRA不会？

2. **实践问题**
   - 如何判断是否需要微调？
   - 微调后模型退化怎么办？

3. **对比分析**
   - 在什么情况下Prompt比微调更好？
   - Full FT vs LoRA如何选择？

---

## 📚 延伸阅读

- 📄 [LoRA: Low-Rank Adaptation of Large Language Models (2021)](https://arxiv.org/abs/2106.09685)
- 📄 [Parameter-Efficient Transfer Learning for NLP (2019)](https://arxiv.org/abs/1902.00751)
- 📄 [The Power of Scale for Parameter-Efficient Prompt Tuning (2021)](https://arxiv.org/abs/2104.08691)
- 📄 [Chain-of-Thought Prompting Elicits Reasoning (2022)](https://arxiv.org/abs/2201.11903)

---

## 🎓 课程总结

恭喜完成《大模型基础》学习笔记！

```
回顾学习路径：
✓ 第1章：理解了LLM的基本概念
✓ 第2章：掌握了Transformer架构
✓ 第3章：学习了训练基础知识
✓ 第4章：了解了微调技术

下一步建议：
→ 实践项目：微调一个小模型
→ 深入学习：阅读经典论文
→ 关注前沿：跟踪最新研究
```

---

[← 上一章：模型训练基础](../ch03-training/README.md) | [返回目录](../../README.md)
