# 可视化示例图库

[← 返回目录](../../README.md)

本文档提供了一些用ASCII艺术和文本图形展示的可视化示例，帮助理解复杂概念。

---

## 📊 Transformer整体架构

```
                    输出概率
                       ↑
                [Linear + Softmax]
                       ↑
              ┌────────────────┐
              │   Decoder      │
              │   Stack (Nx)   │
              │                │
              │  [Multi-Head   │
              │   Attention]   │
              │      ↓         │
              │  [Add & Norm]  │
              │      ↓         │
              │  [Cross        │
              │   Attention]   │
              │      ↓         │
              │  [Add & Norm]  │
              │      ↓         │
              │  [Feed-Forward]│
              │      ↓         │
              │  [Add & Norm]  │
              └────────────────┘
                       ↑
              ┌────────────────┐
              │   Encoder      │
              │   Stack (Nx)   │
              │                │
              │  [Multi-Head   │
              │   Attention]   │
              │      ↓         │
              │  [Add & Norm]  │
              │      ↓         │
              │  [Feed-Forward]│
              │      ↓         │
              │  [Add & Norm]  │
              └────────────────┘
                       ↑
              [Position Encoding]
                       ↑
               [Input Embedding]
                       ↑
                    输入文本
```

---

## 🔍 自注意力计算流程

```
输入序列: ["我", "爱", "学习"]

步骤1: 生成Q, K, V
─────────────────────────────────
      Q          K          V
我  [0.2,...]  [0.1,...]  [0.3,...]
爱  [0.5,...]  [0.4,...]  [0.6,...]
学习 [0.3,...]  [0.2,...]  [0.4,...]


步骤2: 计算注意力分数
─────────────────────────────────
       Q·K^T / √d_k
         我    爱   学习
    我  [2.1  1.5  0.8]
    爱  [1.3  2.5  1.9]
    学习 [0.9  2.1  2.8]


步骤3: Softmax归一化
─────────────────────────────────
      我    爱   学习
我  [0.6  0.3  0.1]  ← 我最关注"我"
爱  [0.2  0.5  0.3]  ← 爱最关注"爱"
学习 [0.1  0.3  0.6]  ← 学习最关注"学习"


步骤4: 加权求和Value
─────────────────────────────────
输出 = 注意力权重 × V
每个词得到融合了上下文的新表示
```

---

## 🎭 多头注意力可视化

```
                   输入 X
                     │
        ┌────────────┼────────────┐
        │            │            │
        ↓            ↓            ↓
    ┌─────┐      ┌─────┐      ┌─────┐
    │Head1│      │Head2│      │Head8│
    │     │      │     │      │     │
    │语法 │      │语义 │      │长距离│
    │关系 │      │关系 │      │依赖 │
    └─────┘      └─────┘      └─────┘
        │            │            │
        └────────────┼────────────┘
                     ↓
                 [Concat]
                     ↓
              [Linear投影]
                     ↓
                   输出


示例分析句子: "猫坐在垫子上"
───────────────────────────────
Head 1 (语法):    Head 2 (语义):
  猫  → 坐        猫  → 垫子
  坐  → 在        坐  → 上
  在  → 上        
  上  → 垫子      

Head 3 (长距离):
  猫  → 垫子 (跨越多个词)
```

---

## 📍 位置编码波形图

```
位置编码的不同维度展示不同频率的波形:

维度0 (高频):
pos: 0  1  2  3  4  5  6  7  8  9
    ╱╲╱╲╱╲╱╲╱╲╱╲╱╲╱╲╱╲╱╲

维度1 (中频):
pos: 0  1  2  3  4  5  6  7  8  9
    ╱‾‾╲__╱‾‾╲__╱‾‾╲__╱

维度2 (低频):
pos: 0  1  2  3  4  5  6  7  8  9
    ╱‾‾‾‾‾╲_____╱‾‾‾‾‾

组合所有维度 → 每个位置有唯一的编码向量
```

---

## 🔄 训练过程可视化

```
训练循环流程:
═════════════════════════════════════

  ┌─────────────────────────────┐
  │  1. 加载一个批次数据         │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  2. 前向传播                │
  │     input → model → output  │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  3. 计算损失                │
  │     loss = f(output, label) │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  4. 反向传播                │
  │     计算所有参数的梯度       │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  5. 梯度裁剪 (可选)         │
  │     防止梯度爆炸            │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  6. 优化器更新参数          │
  │     θ = θ - lr × ∇θ        │
  └──────────┬──────────────────┘
             ↓
  ┌─────────────────────────────┐
  │  7. 学习率调度              │
  │     更新学习率              │
  └──────────┬──────────────────┘
             ↓
             │
      是否训练完成?
         ↓ NO
    ┌────┘
    │
    └──→ 回到步骤1
```

---

## 📈 学习率调度策略

```
1. Warmup + Constant
─────────────────────
LR │     ┌──────────────
   │    ╱
   │   ╱
   │  ╱
   └─────────────────→ Step


2. Warmup + Linear Decay
─────────────────────────
LR │     ┌╲
   │    ╱  ╲
   │   ╱    ╲
   │  ╱      ╲___
   └─────────────────→ Step


3. Warmup + Cosine Decay
─────────────────────────
LR │     ╱‾‾╲___
   │    ╱       ╲___
   │   ╱            ╲__
   │  ╱                ╲
   └─────────────────────→ Step


4. Cosine with Restarts
─────────────────────────
LR │  ╱╲    ╱╲    ╱╲
   │ ╱  ╲  ╱  ╲  ╱  ╲
   │╱    ╲╱    ╲╱    ╲
   └─────────────────────→ Step
     └─┘  └─┘  └─┘
    重启周期
```

---

## 🎨 LoRA工作原理

```
标准微调:
════════════════════════════════
   输入 x                                         
     ↓                                         
  ┌─────┐                                     
  │  W  │ ← 所有参数都要更新 (100%)            
  │4096 │                                     
  │  ×  │                                     
  │4096 │                                     
  └─────┘                                     
     ↓                                         
   输出                                        


LoRA微调:
════════════════════════════════
   输入 x
     │
     ├─────────┬────────→ 
     ↓         ↓         
  ┌─────┐  ┌──────┐    
  │  W  │  │  A   │ ← 只训练A,B (0.1%)
  │冻结🔒│  │4096×8│    
  │     │  └──────┘    
  │     │     ↓        
  │     │  ┌──────┐    
  │     │  │  B   │    
  │     │  │8×4096│    
  └─────┘  └──────┘    
     │         │        
     └────┬────┘        
          ↓             
         相加           
          ↓             
        输出            

W: 预训练权重 (冻结)
A, B: LoRA矩阵 (可训练)
秩 r = 8 << d = 4096
```

---

## 🔌 Adapter结构

```
   输入 (d=768)
       ↓
   ┌────────┐
   │原始层  │ ← 冻结
   └────────┘
       ↓
       │←─────── 残差连接
       ↓         ↑
   ┌────────┐   │
   │Adapter │   │
   │        │   │
   │ ┌────┐ │   │
   │ │Down│ │ d→m (m=64)
   │ └────┘ │   │
   │   ↓   │   │
   │ [ReLU] │   │
   │   ↓   │   │
   │ ┌────┐ │   │
   │ │ Up │ │ m→d
   │ └────┘ │   │
   └────────┘   │
       ↓        │
      相加 ←─────┘
       ↓
    输出

瓶颈设计: d=768 → m=64 → d=768
参数量: 2×768×64 ≈ 98K (vs 768²≈590K)
```

---

## 💬 Prompt模板示例

```
┌─────────────────────────────────────┐
│ System Prompt (系统提示)            │
│ 你是一个专业的AI助手...            │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│ Few-shot Examples (示例)            │
│                                     │
│ 示例1:                              │
│ 输入: ...                          │
│ 输出: ...                          │
│                                     │
│ 示例2:                              │
│ 输入: ...                          │
│ 输出: ...                          │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│ Task Instruction (任务指令)         │
│ 请完成以下任务...                  │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│ Input Data (输入数据)               │
│ 输入: [用户实际输入]                │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│ Output Format (输出格式)            │
│ 输出:                               │
└─────────────────────────────────────┘
```

---

## 🎯 数据流动示意图

```
Token化过程:
═══════════════════════════════════════

文本: "我爱学习人工智能"
  ↓
分词器 (Tokenizer)
  ↓
Tokens: [101, 2769, 4263, 2110, 872, 782, 2339, 6444, 5543, 102]
         ↑                                              ↑
       [CLS]                                          [SEP]
  ↓
Token Embeddings: 每个token → 768维向量
  [0.23, -0.45, 0.67, ..., 0.12]  ← token 101
  [0.11, 0.89, -0.34, ..., -0.56] ← token 2769
  ...
  ↓
Position Embeddings: 加上位置信息
  [0.23, -0.45, ...] + [0.01, 0.02, ...]
  ↓
输入到Transformer
```

---

## 🔢 注意力分数矩阵示例

```
句子: "机器 学习 很 有趣"

注意力热力图 (数值越大，关系越强):
────────────────────────────────────
        机器  学习   很   有趣
机器    ████  ███   ▓    ▓     
学习    ███   ████  ██   ▓     
很      ▓     ██    ███  ███   
有趣    ▓     ▓     ███  ████  

图例:
████ = 0.8-1.0  (强关联)
███  = 0.6-0.8  (较强关联)
██   = 0.4-0.6  (中等关联)
▓    = 0.0-0.4  (弱关联)

解读:
- "机器"和"学习"相互关注度高
- "很"主要关注"有趣"(修饰关系)
- 每个词对自己的关注度最高
```

---

## 📊 模型规模对比

```
模型规模演进:
═══════════════════════════════════

GPT-1     │■ 117M
          │
GPT-2     │■■■ 1.5B
          │
GPT-3     │■■■■■■■■■■■■■■■ 175B
          │
GPT-4     │■■■■■■■■■■■■■■■■■■■■ >1T (估计)
          │
PaLM      │■■■■■■■■■■■■■■■■ 540B
          │
LLaMA-2   │■■■■■■ 70B
          │
          └────────────────────────→
           参数量 (对数刻度)

趋势: 参数量持续增长
     性能也随之提升
```

---

## 🏋️ 训练 vs 推理对比

```
训练阶段:
════════════════════════════════
┌──────────────────────────────┐
│ 输入: 大量文本数据 (TB级)    │
│ 时间: 数周到数月             │
│ 资源: 数百/数千GPU           │
│ 内存: 每GPU数十GB             │
│ 成本: 数百万美元             │
│ 输出: 训练好的模型参数       │
└──────────────────────────────┘


推理阶段:
════════════════════════════════
┌──────────────────────────────┐
│ 输入: 单条query              │
│ 时间: 毫秒到秒级             │
│ 资源: 1个或少量GPU           │
│ 内存: 根据模型大小           │
│ 成本: 较低                   │
│ 输出: 生成的文本             │
└──────────────────────────────┘
```

---

## 🎓 学习路径图

```
大模型学习路径:
═══════════════════════════════════════

      开始
       ↓
   ┌───────┐
   │基础知识│
   │ML/DL  │
   └───┬───┘
       ↓
   ┌───────────┐
   │Transformer│
   │架构理解   │
   └───┬───────┘
       ↓
   ┌──────────┐
   │预训练原理 │
   └───┬──────┘
       ↓
   ┌──────────┐
   │微调技术  │
   │PEFT/LoRA│
   └───┬──────┘
       ↓
   ┌────────────┐
   │Prompt工程  │
   └───┬────────┘
       ↓
   ┌──────────┐
   │实战项目  │
   └───┬──────┘
       ↓
   ┌────────────┐
   │持续学习    │
   │跟踪前沿    │
   └────────────┘
```

---

[← 返回目录](../../README.md)
