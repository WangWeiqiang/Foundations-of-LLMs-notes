# 第一章：大语言模型简介

[← 返回目录](../../README.md) | [下一章：Transformer架构 →](../ch02-transformer/README.md)

---

## 🎯 本章学习目标

- 理解什么是大语言模型（LLM）
- 了解LLM的发展历程
- 掌握核心概念和术语
- 认识LLM的应用场景

---

## 📖 1.1 什么是大语言模型？

### 定义

**大语言模型（Large Language Model, LLM）** 是一种基于深度学习的自然语言处理模型，通过在海量文本数据上进行训练，学习语言的统计规律和语义理解能力。

### 形象理解

```
📚 海量文本数据
    ⬇️ 
🧠 深度神经网络
    ⬇️
💡 理解与生成语言的能力
    ⬇️
✨ 智能对话、文本生成、问答等
```

### 核心特点

| 特点 | 说明 | 例子 |
|------|------|------|
| **大规模参数** | 数十亿到数千亿参数 | GPT-3: 1750亿参数 |
| **预训练** | 在大量无标注数据上学习 | 整个互联网的文本 |
| **泛化能力** | 可迁移到多种下游任务 | 翻译、摘要、问答等 |
| **上下文学习** | 通过示例学习新任务 | Few-shot Learning |

---

## 📅 1.2 发展历程

### 时间线

```
2017  Transformer架构诞生
  |   论文：《Attention is All You Need》
  ⬇️
2018  BERT、GPT-1 发布
  |   双向编码 vs 自回归生成
  ⬇️
2019  GPT-2 发布
  |   15亿参数，展现惊人的文本生成能力
  ⬇️
2020  GPT-3 发布
  |   1750亿参数，Few-shot Learning
  ⬇️
2022  ChatGPT 发布
  |   基于人类反馈的强化学习（RLHF）
  ⬇️
2023  GPT-4、Claude、LLaMA等
  |   多模态、更强推理能力
```

### 关键里程碑

| 年份 | 模型 | 创新点 |
|------|------|--------|
| 2017 | Transformer | 自注意力机制，取代RNN |
| 2018 | BERT | 双向预训练，掩码语言模型 |
| 2018 | GPT-1 | 生成式预训练 |
| 2020 | GPT-3 | 上下文学习能力 |
| 2022 | ChatGPT | RLHF对齐人类偏好 |

---

## 🔑 1.3 核心概念与术语

### 3.1 预训练 (Pre-training)

在大规模无标注文本上训练模型，让模型学习语言的基本规律。

```
📊 训练数据示例：
输入：  "今天天气真___"
目标：  预测下一个词 "好"

通过预测下一个词，模型学习：
✓ 语法结构
✓ 词汇搭配
✓ 常识知识
✓ 推理能力
```

### 3.2 微调 (Fine-tuning)

在特定任务的标注数据上继续训练，使模型适应特定应用。

```
🎯 任务示例：情感分析

输入：  "这部电影真的太好看了！"
标签：  正面情感 ✓

预训练模型 ➡️ 微调 ➡️ 情感分析专家
```

### 3.3 提示 (Prompt)

给模型的输入指令或问题，引导模型生成期望的输出。

```
💬 Prompt工程示例：

❌ 弱提示：
   "翻译：Hello"

✅ 强提示：
   "请将以下英文翻译成中文，保持语气和含义：
    英文：Hello, how are you today?
    中文："
```

### 3.4 上下文窗口 (Context Window)

模型一次能处理的最大文本长度。

```
📏 不同模型的上下文窗口：

GPT-3:      4,096 tokens  (约3,000字)
GPT-4:      8,192 tokens  (约6,000字)
Claude-2:   100,000 tokens (约75,000字)

1 token ≈ 0.75 个英文单词
1 token ≈ 0.5-1 个中文字符
```

### 3.5 Token化 (Tokenization)

将文本分割成模型可以处理的基本单元。

```
🔤 Token化示例：

文本: "大语言模型很强大"

字节对编码(BPE)可能的分割：
["大", "语言", "模型", "很", "强大"]

或更细粒度：
["大", "语", "言", "模", "型", "很", "强", "大"]

不同的分割方式影响：
- 词汇表大小
- 训练效率
- 模型性能
```

---

## 🌟 1.4 应用场景

### 4.1 文本生成

```
📝 应用：
- 文章写作
- 代码生成
- 创意创作
- 邮件撰写

示例：
输入：  "写一首关于春天的诗"
输出：  "春风拂面暖，
        桃花朵朵开。
        鸟鸣枝头跃，
        万物复苏来。"
```

### 4.2 问答系统

```
❓ 应用：
- 智能客服
- 搜索引擎
- 知识问答
- 教育辅导

示例：
问：什么是注意力机制？
答：注意力机制是一种让模型关注输入中重要部分的技术...
```

### 4.3 文本理解

```
🔍 应用：
- 情感分析
- 文本分类
- 命名实体识别
- 关键信息提取

示例：
输入：  "这个产品质量很好，物流也快"
分析：  情感-正面, 提及-产品+物流
```

### 4.4 机器翻译

```
🌐 应用：
- 多语言翻译
- 方言转换
- 古文翻译

示例：
EN: "Artificial Intelligence is transforming the world"
CN: "人工智能正在改变世界"
```

---

## 💡 关键要点总结

### 核心要点

1. **LLM = 大规模 + 预训练 + Transformer**
   - 数十亿参数
   - 海量文本训练
   - 自注意力机制

2. **训练范式：预训练 → 微调**
   - 预训练：学习通用语言能力
   - 微调：适应特定任务

3. **核心能力**
   - 文本生成
   - 语言理解
   - 上下文学习
   - 知识问答

4. **关键概念**
   - Token化：文本单元化
   - Prompt：输入设计
   - Context：上下文长度

### 学习路径

```
第1章：理解什么是LLM (当前) ✓
   ⬇️
第2章：学习Transformer架构
   ⬇️
第3章：掌握训练方法
   ⬇️
第4章：应用微调技术
```

---

## 🎓 思考题

1. **概念理解**
   - 大语言模型的"大"体现在哪些方面？
   - 预训练和微调的区别是什么？

2. **实践思考**
   - 为什么需要Token化？直接用字符不行吗？
   - 上下文窗口的长度对模型有什么影响？

3. **应用场景**
   - 你能想到LLM还可以应用在哪些场景？
   - 如何设计一个好的Prompt？

---

## 📚 延伸阅读

- 📄 论文：[Attention is All You Need](https://arxiv.org/abs/1706.03762)
- 📄 论文：[Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- 🔗 博客：[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

---

[← 返回目录](../../README.md) | [下一章：Transformer架构 →](../ch02-transformer/README.md)
