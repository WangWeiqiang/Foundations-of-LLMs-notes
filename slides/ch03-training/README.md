# ç¬¬ä¸‰ç« ï¼šæ¨¡å‹è®­ç»ƒåŸºç¡€

[â† ä¸Šä¸€ç« ï¼šTransformeræ¶æ„](../ch02-transformer/README.md) | [è¿”å›ç›®å½•](../../README.md) | [ä¸‹ä¸€ç« ï¼šå¾®è°ƒæŠ€æœ¯ â†’](../ch04-finetuning/README.md)

---

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡

- ç†è§£è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç›®æ ‡
- æŒæ¡æŸå¤±å‡½æ•°çš„è®¾è®¡
- äº†è§£åå‘ä¼ æ’­ç®—æ³•
- å­¦ä¹ ä¼˜åŒ–å™¨çš„é€‰æ‹©å’Œä½¿ç”¨

---

## ğŸ“Š 3.1 è®­ç»ƒç›®æ ‡

### è¯­è¨€æ¨¡å‹çš„æœ¬è´¨

```
ğŸ¯ æ ¸å¿ƒä»»åŠ¡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯

ç»™å®šä¸Šæ–‡ï¼šP(word_n | word_1, word_2, ..., word_{n-1})

ä¾‹å­ï¼š
è¾“å…¥ï¼š "ä»Šå¤©å¤©æ°”çœŸ"
æ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼š
  å¥½:   0.45  â¬…ï¸ æœ€å¯èƒ½
  ä¸é”™: 0.30
  å·®:   0.10
  ...
```

### è‡ªå›å½’è¯­è¨€æ¨¡å‹

```
ğŸ“ è®­ç»ƒæ–¹å¼ï¼š

æ–‡æœ¬: "æˆ‘ çˆ± å­¦ä¹  å¤§è¯­è¨€æ¨¡å‹"

è®­ç»ƒæ ·æœ¬ç”Ÿæˆï¼š
è¾“å…¥: "æˆ‘"           â†’ ç›®æ ‡: "çˆ±"
è¾“å…¥: "æˆ‘ çˆ±"        â†’ ç›®æ ‡: "å­¦ä¹ "
è¾“å…¥: "æˆ‘ çˆ± å­¦ä¹ "   â†’ ç›®æ ‡: "å¤§è¯­è¨€æ¨¡å‹"
         â¬‡ï¸
    é€è¯é¢„æµ‹ï¼Œè‡ªå›å½’ç”Ÿæˆ
```

### è®­ç»ƒç›®æ ‡å…¬å¼

```
æœ€å¤§åŒ–ä¼¼ç„¶ï¼š
max Î£ log P(xáµ¢ | xâ‚, xâ‚‚, ..., xáµ¢â‚‹â‚)
    i

é€šä¿—ç†è§£ï¼š
è®©æ¨¡å‹ç»™æ­£ç¡®ç­”æ¡ˆæ›´é«˜çš„æ¦‚ç‡
```

---

## ğŸ”¢ 3.2 æŸå¤±å‡½æ•°

### äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)

#### åŸç†

```
ğŸ“ å…¬å¼ï¼š
Loss = -Î£ yáµ¢ Ã— log(Å·áµ¢)
        i

å…¶ä¸­ï¼š
- yï¼š  çœŸå®æ ‡ç­¾ï¼ˆone-hotç¼–ç ï¼‰
- Å·ï¼š æ¨¡å‹é¢„æµ‹æ¦‚ç‡

ä¾‹å­ï¼š
çœŸå®è¯ï¼š  "å¥½"
è¯æ±‡è¡¨ï¼š  [å¥½, å, å¯ä»¥, ...]

çœŸå®åˆ†å¸ƒ y:   [1,   0,   0,   ...]
æ¨¡å‹é¢„æµ‹ Å·:   [0.7, 0.2, 0.1, ...]

Loss = -1Ã—log(0.7) - 0Ã—log(0.2) - 0Ã—log(0.1)
     = -log(0.7) 
     â‰ˆ 0.357
```

#### ç›´è§‚ç†è§£

```
ğŸ’¡ æŸå¤±å‡½æ•°çš„å«ä¹‰ï¼š

æ¨¡å‹é¢„æµ‹     æŸå¤±å€¼      è§£é‡Š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Å· = 0.9     -log(0.9) = 0.11   é¢„æµ‹å¾ˆå‡†ç¡®
Å· = 0.5     -log(0.5) = 0.69   é¢„æµ‹ä¸ç¡®å®š
Å· = 0.1     -log(0.1) = 2.30   é¢„æµ‹å¾ˆå·®

è§„å¾‹ï¼š
é¢„æµ‹æ¦‚ç‡è¶Šé«˜ â†’ æŸå¤±è¶Šå° âœ“
é¢„æµ‹æ¦‚ç‡è¶Šä½ â†’ æŸå¤±è¶Šå¤§ âœ—
```

### å›°æƒ‘åº¦ (Perplexity)

```
ğŸ“Š è¯„ä¼°æŒ‡æ ‡ï¼š
PPL = exp(Loss)

ç†è§£ï¼š
å›°æƒ‘åº¦ = æ¨¡å‹æœ‰å¤š"å›°æƒ‘"

ä¾‹å­ï¼š
Loss = 2.0  â†’ PPL = eÂ² â‰ˆ 7.4
æ„æ€ï¼šæ¨¡å‹åœ¨7-8ä¸ªå€™é€‰è¯ä¸­çŠ¹è±«

Loss = 1.0  â†’ PPL = eÂ¹ â‰ˆ 2.7
æ„æ€ï¼šæ¨¡å‹åœ¨2-3ä¸ªå€™é€‰è¯ä¸­çŠ¹è±«

å›°æƒ‘åº¦è¶Šä½ â†’ æ¨¡å‹è¶Šç¡®å®š â†’ æ€§èƒ½è¶Šå¥½
```

---

## ğŸ”„ 3.3 åå‘ä¼ æ’­ç®—æ³•

### è®¡ç®—å›¾

```
ğŸ”€ å‰å‘ä¼ æ’­ï¼š
è¾“å…¥ â†’ Embedding â†’ Transformer â†’ Softmax â†’ è¾“å‡º
 x       E            h            p        Å·

ğŸ“Š è®¡ç®—æŸå¤±ï¼š
Loss = CrossEntropy(Å·, y)

â¬…ï¸ åå‘ä¼ æ’­ï¼š
è®¡ç®—æ¢¯åº¦ï¼Œä»åå‘å‰ä¼ é€’
```

### é“¾å¼æ³•åˆ™

```
ğŸ§® æ¢¯åº¦è®¡ç®—ï¼š

âˆ‚Loss   âˆ‚Loss   âˆ‚Å·
â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€ Ã— â”€â”€â”€
 âˆ‚W      âˆ‚Å·     âˆ‚W

å½¢è±¡ç†è§£ï¼š
å°±åƒå¤šç±³è¯ºéª¨ç‰Œï¼Œä»åå¾€å‰æ¨å€’
æ¯ä¸€å±‚çš„æ¢¯åº¦ = åä¸€å±‚æ¢¯åº¦ Ã— æœ¬å±‚å¯¼æ•°
```

### æ¢¯åº¦æµåŠ¨

```
ğŸ“‰ æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š

è¾“å‡ºå±‚ â”€â”€â”€â”€â”€â”€â”€â”€â†’ ä¸­é—´å±‚ â”€â”€â”€â”€â”€â”€â”€â”€â†’ è¾“å…¥å±‚
 (æ¢¯åº¦å¤§)       (æ¢¯åº¦å˜å°)      (æ¢¯åº¦å¾ˆå°)

åŸå› ï¼š
å¤šæ¬¡çŸ©é˜µä¹˜æ³• â†’ æ¢¯åº¦æŒ‡æ•°è¡°å‡

è§£å†³æ–¹æ¡ˆï¼š
âœ“ æ®‹å·®è¿æ¥ (Residual Connection)
âœ“ Layer Normalization
âœ“ åˆé€‚çš„å­¦ä¹ ç‡
âœ“ æ¢¯åº¦è£å‰ª
```

---

## âš™ï¸ 3.4 ä¼˜åŒ–å™¨

### SGD (éšæœºæ¢¯åº¦ä¸‹é™)

```
ğŸ“ æ›´æ–°å…¬å¼ï¼š
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· Ã— âˆ‡L(Î¸â‚œ)

å…¶ä¸­ï¼š
- Î¸ï¼šå‚æ•°
- Î·ï¼šå­¦ä¹ ç‡
- âˆ‡Lï¼šæ¢¯åº¦

ç‰¹ç‚¹ï¼š
âœ“ ç®€å•ç›´æ¥
âœ— æ”¶æ•›æ…¢
âœ— éœ€è¦ç²¾å¿ƒè°ƒæ•´å­¦ä¹ ç‡
```

### Momentum

```
ğŸš€ åŠ å…¥åŠ¨é‡ï¼š
vâ‚œ = Î² Ã— vâ‚œâ‚‹â‚ + âˆ‡L(Î¸â‚œ)
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· Ã— vâ‚œ

å½¢è±¡ç†è§£ï¼š
åƒæ»šé›ªçƒï¼Œç§¯ç´¯ä¹‹å‰çš„æ–¹å‘
å¯ä»¥çªç ´å±€éƒ¨æœ€ä¼˜

å‚æ•°ï¼š
Î² = 0.9 (å¸¸ç”¨å€¼)
```

### Adam (æ¨è)

```
ğŸ¯ è‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

mâ‚œ = Î²â‚ Ã— mâ‚œâ‚‹â‚ + (1-Î²â‚) Ã— âˆ‡L     (ä¸€é˜¶çŸ©ä¼°è®¡)
vâ‚œ = Î²â‚‚ Ã— vâ‚œâ‚‹â‚ + (1-Î²â‚‚) Ã— (âˆ‡L)Â² (äºŒé˜¶çŸ©ä¼°è®¡)

mÌ‚â‚œ = mâ‚œ / (1 - Î²â‚áµ—)  (åå·®ä¿®æ­£)
vÌ‚â‚œ = vâ‚œ / (1 - Î²â‚‚áµ—)

Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· Ã— mÌ‚â‚œ / (âˆšvÌ‚â‚œ + Îµ)

å‚æ•°è®¾ç½®ï¼š
Î²â‚ = 0.9    (åŠ¨é‡ç³»æ•°)
Î²â‚‚ = 0.999  (äºŒé˜¶çŸ©ç³»æ•°)
Îµ  = 1e-8   (æ•°å€¼ç¨³å®šé¡¹)
Î·  = 1e-4   (å­¦ä¹ ç‡)
```

#### Adamä¼˜åŠ¿

```
âœ… ä¼˜ç‚¹ï¼š
1. è‡ªé€‚åº”å­¦ä¹ ç‡
2. å¯¹æ¯ä¸ªå‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
3. å¯¹å™ªå£°æ¢¯åº¦é²æ£’
4. è¶…å‚æ•°ç›¸å¯¹ä¸æ•æ„Ÿ

ğŸ’¡ å½¢è±¡ç†è§£ï¼š
SGDï¼š    æ‰€æœ‰å‚æ•°ç”¨ç›¸åŒæ­¥é•¿å‰è¿›
Adamï¼š   æ¯ä¸ªå‚æ•°æ ¹æ®åœ°å½¢è°ƒæ•´æ­¥é•¿
```

### AdamW

```
ğŸ”§ æ”¹è¿›ç‰ˆAdamï¼š
åœ¨AdamåŸºç¡€ä¸Šæ­£ç¡®å®ç°æƒé‡è¡°å‡

æ›´æ–°å…¬å¼ï¼š
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· Ã— (mÌ‚â‚œ/(âˆšvÌ‚â‚œ + Îµ) + Î»Ã—Î¸â‚œ)
                                    â¬†ï¸
                                æƒé‡è¡°å‡é¡¹

ä¼˜åŠ¿ï¼š
âœ“ æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
âœ“ ä¸å­¦ä¹ ç‡è§£è€¦
âœ“ å¤§æ¨¡å‹è®­ç»ƒçš„é¦–é€‰

æ¨èå‚æ•°ï¼š
Î» = 0.01  (æƒé‡è¡°å‡ç³»æ•°)
```

---

## ğŸ“ˆ 3.5 å­¦ä¹ ç‡è°ƒåº¦

### ä¸ºä»€ä¹ˆéœ€è¦è°ƒåº¦ï¼Ÿ

```
ğŸ¯ è®­ç»ƒä¸åŒé˜¶æ®µçš„éœ€æ±‚ï¼š

è®­ç»ƒåˆæœŸï¼šéœ€è¦å¤§æ­¥é•¿ï¼Œå¿«é€Ÿé€¼è¿‘æœ€ä¼˜
è®­ç»ƒåæœŸï¼šéœ€è¦å°æ­¥é•¿ï¼Œç²¾ç¡®æ”¶æ•›

å›ºå®šå­¦ä¹ ç‡çš„é—®é¢˜ï¼š
- å¤ªå¤§ï¼šåæœŸéœ‡è¡ï¼Œæ— æ³•æ”¶æ•›
- å¤ªå°ï¼šå‰æœŸæ…¢ï¼Œè®­ç»ƒæ—¶é—´é•¿
```

### Warmupç­–ç•¥

```
ğŸ“Š é¢„çƒ­é˜¶æ®µï¼š

å­¦ä¹ ç‡
  â¬†ï¸
  â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚    â•±
  â”‚   â•±
  â”‚  â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ è®­ç»ƒæ­¥æ•°
    Warmup  ç¨³å®šé˜¶æ®µ

å…¬å¼ï¼š
Î· = Î·_max Ã— min(step/warmup_steps, 1)

ä½œç”¨ï¼š
- é¿å…è®­ç»ƒåˆæœŸæ¢¯åº¦çˆ†ç‚¸
- è®©æ¨¡å‹"é¢„çƒ­"
- ç¨³å®šè®­ç»ƒè¿‡ç¨‹

å…¸å‹è®¾ç½®ï¼š
warmup_steps = 4000 æˆ– 10000
```

### Cosineè¡°å‡

```
ğŸ“‰ ä½™å¼¦é€€ç«ï¼š

å­¦ä¹ ç‡
  â¬†ï¸
  â”‚â•²
  â”‚ â•²___
  â”‚     â•²___
  â”‚         â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ è®­ç»ƒæ­¥æ•°

å…¬å¼ï¼š
Î· = Î·_min + 0.5Ã—(Î·_max - Î·_min)Ã—(1 + cos(Ï€Ã—t/T))

ç‰¹ç‚¹ï¼š
- å¹³æ»‘è¡°å‡
- åæœŸå­¦ä¹ ç‡æ¥è¿‘0
- æœ‰åˆ©äºæ”¶æ•›

æµè¡Œå˜ä½“ï¼š
- Cosine Annealing with Restarts
- Cosine Decay with Warmup
```

### é˜¶æ¢¯è¡°å‡

```
ğŸ“Š åˆ†æ®µå¸¸æ•°ï¼š

å­¦ä¹ ç‡
  â¬†ï¸
  â”‚â”€â”€â”€â”€â”
  â”‚    â”‚
  â”‚    â””â”€â”€â”€â”€â”
  â”‚         â”‚
  â”‚         â””â”€â”€â”€â”€
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ epoch

æ¯Nä¸ªepoché™ä½ä¸€æ¬¡å­¦ä¹ ç‡ï¼š
Î· = Î·_0 Ã— Î³^(epoch/N)

ä¾‹å¦‚ï¼š
Î·_0 = 1e-3
Î³ = 0.1
N = 30

Epoch 0-29:   Î· = 1e-3
Epoch 30-59:  Î· = 1e-4
Epoch 60-89:  Î· = 1e-5
```

---

## ğŸ® 3.6 è®­ç»ƒæŠ€å·§

### æ¢¯åº¦è£å‰ª

```
âœ‚ï¸ é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼š

if ||g|| > threshold:
    g = g Ã— (threshold / ||g||)

ä¾‹å­ï¼š
æ¢¯åº¦èŒƒæ•° = 100
é˜ˆå€¼ = 1.0
è£å‰ªå = 1.0

ä½œç”¨ï¼š
- é™åˆ¶æ¢¯åº¦å¤§å°
- ç¨³å®šè®­ç»ƒ
- é˜²æ­¢å‚æ•°çªå˜

æ¨èå€¼ï¼š
threshold = 1.0
```

### Dropout

```
ğŸ² éšæœºå¤±æ´»ï¼š

è®­ç»ƒæ—¶ï¼š
éšæœºå…³é—­éƒ¨åˆ†ç¥ç»å…ƒï¼ˆå¦‚50%ï¼‰

æµ‹è¯•æ—¶ï¼š
ä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒï¼Œè¾“å‡ºä¹˜ä»¥ä¿ç•™ç‡

ä½œç”¨ï¼š
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- å¢å¼ºæ³›åŒ–èƒ½åŠ›
- ç±»ä¼¼é›†æˆå­¦ä¹ 

ä»£ç ç¤ºä¾‹ï¼ˆæ¦‚å¿µï¼‰ï¼š
if training:
    mask = random(shape) > dropout_rate
    output = input * mask / (1 - dropout_rate)
else:
    output = input
```

### Batch Sizeé€‰æ‹©

```
ğŸ“¦ æ‰¹æ¬¡å¤§å°çš„å½±å“ï¼š

å°æ‰¹æ¬¡ (32-128):
âœ“ æ¢¯åº¦å™ªå£°å¤§ï¼Œå¸®åŠ©è·³å‡ºå±€éƒ¨æœ€ä¼˜
âœ“ å†…å­˜å ç”¨å°
âœ— è®­ç»ƒæ…¢
âœ— ä¸ç¨³å®š

å¤§æ‰¹æ¬¡ (1024-4096):
âœ“ è®­ç»ƒå¿«
âœ“ æ¢¯åº¦å‡†ç¡®
âœ— å†…å­˜å ç”¨å¤§
âœ— å¯èƒ½é™·å…¥å°–é”æœ€ä¼˜ï¼ˆæ³›åŒ–å·®ï¼‰

æ¨èç­–ç•¥ï¼š
- æ ¹æ®GPUå†…å­˜é€‰æ‹©
- å¤§batché…åˆå­¦ä¹ ç‡çº¿æ€§ç¼©æ”¾
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batch
```

### æ¢¯åº¦ç´¯ç§¯

```
ğŸ”„ æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡ï¼š

for i in range(accumulation_steps):
    loss = forward(batch_i) / accumulation_steps
    loss.backward()  # ç´¯ç§¯æ¢¯åº¦

optimizer.step()     # æ›´æ–°ä¸€æ¬¡
optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦

æ•ˆæœï¼š
batch_size=32, steps=4 
ç­‰æ•ˆäº batch_size=128

ä¼˜åŠ¿ï¼š
- åœ¨æœ‰é™å†…å­˜ä¸‹è®­ç»ƒå¤§æ‰¹æ¬¡
- æé«˜è®­ç»ƒç¨³å®šæ€§
```

---

## ğŸ“‹ 3.7 è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒå¾ªç¯

```python
# ä¼ªä»£ç å±•ç¤ºè®­ç»ƒæµç¨‹
for epoch in range(num_epochs):
    for batch in dataloader:
        # 1. å‰å‘ä¼ æ’­
        logits = model(batch.input)
        loss = criterion(logits, batch.target)
        
        # 2. åå‘ä¼ æ’­
        loss.backward()
        
        # 3. æ¢¯åº¦è£å‰ª
        clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 4. ä¼˜åŒ–å™¨æ›´æ–°
        optimizer.step()
        optimizer.zero_grad()
        
        # 5. å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # 6. æ—¥å¿—è®°å½•
        if step % log_interval == 0:
            print(f"Loss: {loss:.4f}, PPL: {exp(loss):.2f}")
```

### æ£€æŸ¥ç‚¹ä¿å­˜

```
ğŸ’¾ ä¿å­˜ç­–ç•¥ï¼š

1. å®šæœŸä¿å­˜ï¼š
   æ¯Næ­¥ä¿å­˜ä¸€æ¬¡
   checkpoint_1000.pt
   checkpoint_2000.pt

2. æœ€ä½³æ¨¡å‹ï¼š
   éªŒè¯é›†å›°æƒ‘åº¦æœ€ä½æ—¶ä¿å­˜
   best_model.pt

3. ä¿å­˜å†…å®¹ï¼š
   - æ¨¡å‹å‚æ•°
   - ä¼˜åŒ–å™¨çŠ¶æ€
   - å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
   - è®­ç»ƒæ­¥æ•°
   - éšæœºç§å­
```

---

## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

### è®­ç»ƒæ ¸å¿ƒ

1. **æŸå¤±å‡½æ•° = äº¤å‰ç†µ**
   ```
   è®©æ¨¡å‹ç»™æ­£ç¡®ç­”æ¡ˆæ›´é«˜çš„æ¦‚ç‡
   ```

2. **ä¼˜åŒ–å™¨ = AdamW**
   ```
   è‡ªé€‚åº”å­¦ä¹ ç‡ + æƒé‡è¡°å‡
   ```

3. **å­¦ä¹ ç‡è°ƒåº¦ = Warmup + Cosine**
   ```
   é¢„çƒ­ â†’ ç¨³å®š â†’ è¡°å‡
   ```

4. **è®­ç»ƒæŠ€å·§**
   ```
   æ¢¯åº¦è£å‰ª + Dropout + æ¢¯åº¦ç´¯ç§¯
   ```

### æ¨èé…ç½®

```
ğŸ“‹ æ ‡å‡†é…ç½®ï¼ˆå‚è€ƒGPT-2ï¼‰ï¼š

ä¼˜åŒ–å™¨ï¼š     AdamW
å­¦ä¹ ç‡ï¼š     6e-4
Warmupï¼š     2000 steps
è¡°å‡ç­–ç•¥ï¼š   Cosine
Batch sizeï¼š  0.5M tokens
æ¢¯åº¦è£å‰ªï¼š   1.0
æƒé‡è¡°å‡ï¼š   0.1
Dropoutï¼š    0.1
```

---

## ğŸ¯ æ€è€ƒé¢˜

1. **æ¦‚å¿µç†è§£**
   - ä¸ºä»€ä¹ˆç”¨äº¤å‰ç†µè€Œä¸æ˜¯MSEï¼Ÿ
   - å›°æƒ‘åº¦å’ŒæŸå¤±çš„å…³ç³»ï¼Ÿ

2. **è®­ç»ƒæŠ€å·§**
   - æ¢¯åº¦è£å‰ªå’Œæ¢¯åº¦ç´¯ç§¯çš„åŒºåˆ«ï¼Ÿ
   - å¤§batchä¸ºä»€ä¹ˆå¯èƒ½å½±å“æ³›åŒ–ï¼Ÿ

3. **å®è·µé—®é¢˜**
   - å¦‚ä½•åˆ¤æ–­å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼Ÿ
   - è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- ğŸ“„ [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
- ğŸ“„ [Decoupled Weight Decay Regularization (AdamW)](https://arxiv.org/abs/1711.05101)
- ğŸ“ [An Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/)

---

[â† ä¸Šä¸€ç« ï¼šTransformeræ¶æ„](../ch02-transformer/README.md) | [è¿”å›ç›®å½•](../../README.md) | [ä¸‹ä¸€ç« ï¼šå¾®è°ƒæŠ€æœ¯ â†’](../ch04-finetuning/README.md)
